Getting a prediction model for 2012
===================================

```{r options, echo=FALSE}
opts_chunk$set(cache=TRUE)
```

```{r setup}
## Load data
load("../../data/process/first.Rdata")
load("../../data/process/second.Rdata")
load("../../data/process/full.Rdata")
load("../../data/process/gameFirst.Rdata")
load("../../data/process/gameFull.Rdata")
load("../../data/pred/info2012.Rdata")

## Fix names
names(gameFirst) <- names(first)
names(gameFull) <- names(full)

## Specify data to use when to train the model
gameFi2011 <- do.call(rbind, gameFirst[as.character(2006:2011)])

## Transform to factors so glm() won't complain
gameFi2011$teamA <- as.factor(gameFi2011$teamA)
gameFi2011$teamB <- as.factor(gameFi2011$teamB)

info2012$teamA <- as.factor(info2012$teamA)
info2012$teamB <- as.factor(info2012$teamB)

## Load libraries
```

```{r train, dependson="setup"}

## Fit with all the variables
fit <- glm(win ~ . - teamA -teamB, family=binomial, data=gameFi2011)

## Explore the results
summary(fit)

## do the single term deletions
drop1(fit, test="Chi")
```

```{r step, dependson=c("setup", "train")}
## What is the model when using step-wise selection?
fitStep <- step(fit, trace=0)
summary(fitStep)
```

Lets see how the predictions for 2012 using the first half data of those games and the model selected through step.

```{r pred, dependson=c("setup", "step")}

pResp <- predict(fitStep, newdata=info2012, se=TRUE, type="response")
plot(pResp$fit, pResp$se)

plot(pResp$fit, info2012$win)

p <- seq(0, 1, by=0.001)
idx <- rep(c(TRUE, FALSE), nrow(info2012)/2)

acc <- sapply(p, function(x) { 
	i <- which(pResp$fit <= x)
	sum(info2012$win[i]) / length(i)
})

acc[is.nan(acc) | acc == Inf] <- 0
plot(p, acc)
abline(a=0, b=1, col="red")

```

```{r brianWay, dependson=c("setup", "step", "pred")}

ilogit <- function(x) { exp(x)/(1+exp(x))}
paired <- sapply(which(idx), function(i) {
	logitA <- predict(fitStep, newdata=info2012[i, ])
	logitB <- predict(fitStep, newdata=info2012[i+1, ])
	p1 <- ilogit(logitA - logitB)
	return(c(p1, 1-p1))
})
paired <- as.vector(paired)

acc2 <- sapply(p, function(x) { 
	i <- which(paired <= x)
	sum(info2012$win[i]) / length(i)
})
acc2[is.nan(acc2) | acc2 == Inf] <- 0
plot(p, acc2)
abline(a=0, b=1, col="red")

## Repeat with full model
paired2 <- sapply(which(idx), function(i) {
	logitA <- predict(fit, newdata=info2012[i, ])
	logitB <- predict(fit, newdata=info2012[i+1, ])
	p1 <- ilogit(logitA - logitB)
	return(c(p1, 1-p1))
})
paired2 <- as.vector(paired2)

acc3 <- sapply(p, function(x) { 
	i <- which(paired2 <= x)
	sum(info2012$win[i]) / length(i)
})
acc3[is.nan(acc3) | acc3 == Inf] <- 0
plot(p, acc3)
abline(a=0, b=1, col="red")
```

I think that I might be doing something wrong, let me trace back

```{r compareAcc, dependson=c("setup", "step", "pred", "brainWay")}
plot(p, acc, ylim=range(c(acc, acc2, acc3)), type="l", lwd=2)
lines(p, acc2, col="blue", lwd=2)
lines(p, acc3, col="orange", lwd=2)
abline(a=0, b=1, col="red")
```


```{r BBurke, dependson=c("setup", "step")}

source("predFunctions.R")

## well, this is actually the same as "paired" as shown a few lines below
pStep <- getPred(fitStep, info2012)

## Set number of bins
bin <- 30

## Yes, same as paired
pStep.eval <- evalPred(pStep, bin, info2012)
sum(evalPred(paired, bin, info2012, plot=FALSE)$real - pStep.eval$real)

pResp.eval <- evalPred(pResp$fit, bin, info2012)
plot(pStep.eval$centers, pStep.eval$real - pResp.eval$real)

plot(pStep.eval$centers, pStep.eval$real, type="o", col="blue", ylim=c(0, 1))
lines(pStep.eval$centers, pResp.eval$real, type="o", col="orange")
abline(a=0, b=1, col="red")

## Seems better to use regular predict() than to do the diffence in logits
## However, in such a case the probabilities don't add to 1!!!

hist(pResp$fit[idx] + pResp$fit[!idx])
table(pStep[idx] + pStep[!idx])


## What if I don't use the paired info?
check <- pStep[idx]
check.eval <- evalPred(check, bin, info2012[idx,])

plot(pStep.eval$centers, pStep.eval$real, type="o", col="blue", ylim=c(0, 1))
lines(pStep.eval$centers, check.eval$real, type="o", col="orange")
abline(a=0, b=1, col="red")
## Plot looks more jumpy, should be because of less data
```


Ok, now it looks like I got this right.

```{r export, dependson=c("setup", "step")}
## Re-train a model now with 2006 to 2012 data for predicting 2013 games

## Specify data to use when to train the model
gameFi2012 <- do.call(rbind, gameFirst[as.character(2006:2012)])

## Transform to factors so glm() won't complain
gameFi2012$teamA <- as.factor(gameFi2012$teamA)
gameFi2012$teamB <- as.factor(gameFi2012$teamB)

f2013 <- glm(win ~ teamAoPassYdsAtt + teamAoRun + teamAdPassYdsAtt + 
    teamAdRunAtt + teamBoRun + teamBoFumble + teamBdRunAtt + 
    local + halfdiff + resumes + gwrA + gwrB, family = binomial, 
    data = gameFi2012)
	
## Lets look at it
summary(f2013)

## Save the trained models
fits <- list("2012"=fitStep, "2013"=f2013)
save(fits, file="fits.Rdata")
```




